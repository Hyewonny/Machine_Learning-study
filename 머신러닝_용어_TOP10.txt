1. 휴리스틱
불충분한 시간이나 정보로 인하여 합리적인 판단을 할 수 없거나, 
사람들이 빠르게 사용할 수 있는 어림짐작의 방법이다.
복잡한 문제나 불완전한 정보에 직면해서 의사결정을 하고 
판단을 내리고 문제를 해결하는 방법을 설명하기 위해 제안되어 왔던 간단하고 효율적인 주먹구구의 규칙들 이라고 할 수있음
문제 예시 -> 다음 중, 휴리스틱한 의사결정 과정이 아닌 것은 ?

2.가우시안분포
표준정규분포 정규분포 밀도 함수에서 를 통해 X를 Z로 정규화함으로써 평균이 0, 표준편차가 1인 표준정규분포를 얻을 수 있다.

3.베르누이 분포
확률론과 통계학에서 매 시행마다 오직 두 가지의 가능한 결과만 일어난다고 할 때, 이러한 실험을 1회 시행하여 일어난 두 가지 결과에 의해 
그 값이 각각 0과 1로 결정되는 확률변수 X에 대한 분포

4.CrossValidation (CV)
교차검증이라는 뜻, 과최적화가 일어나면 트레이닝 데이터에 대해서는 예측이 잘 되지만 테스트 데이터에 대해서는 예측 성능이 급격히 떨어지는 현상이 발생한다.
따라서 모형의 최종 성능을 객관적으로 측정하려면 트레이닝에 사용되지 않은 새로운 데이터, 
즉 검증용 혹은 테스트용 데이터를 사용해서 예측한 결과를 기반으로 성능을 계산해야 한다는 의미.

5.A/B TEST
마케팅에서 두 가지 이상의 시안 중 최적안을 선정하기 위해 시험하는 방법
단순 아웃풋에 대한 평가 척도로 인과 관계를 설명해줄수는 없다.

6.MLE 최대우도(Maximum Likelihood Estimation)
최대가능도라고도하며, 어떤 확률변수에서 표집한 값들을 토대로 그 확률변수의 모수를 구하는 방법이다.
어떤 모수가 주어졌을 떄, 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법이며, 점추정 방식에 속한다.

7.MAP 최대 사후 확률 (Maximum A Posteriori)
베이즈 통계학에서 사후 확률의 최빈값을 가리킨다. 
최대 사후 확률 모수는 모수의 사전 확률과 결합된 확률을 고려한다는 점이 MLE와 다르다.
(여기서 사전확률이란 특정 사상이 일어나기 전의 확률, 관측을 하기 전에 가지고 있는 선험적인 확률 분포를 의미함)
사전확률과 가능도가 주어졌을 떄, 관측자는 관측값을 얻은 다음 베이즈 정리에 의해 사후확률을 얻을 수 있다.

8. R-Squared(결정계수)/ Adjusted R-Squared
추정한 선형 모형이 주어진 자료에 적합한 정도를 재는 척도이다.
반응 변수의 변동량 중에서 적용한 모형으로 설명가능한 부분의 비율을 가리킨다.
일반적으로 모형의 설명력으로 해석되짐나 모형에 설명 변수가 들어갈수록 증가하기 때문에 해서에 주의해야 하며,
이러한 문제를 해결하기 위해 Adjusted R-Squared 가 제시되었다. 결정계수의 값은 0에서 1 사이에 있으며, 
종속변인과 독립변인 사이에 상관관계가 높을수록 1에 가까워진다. 즉, 결정계수가 0에 가까운 값을 가지느 회귀모형은 유용성이 낮은 반면,
결정계수의 값이 클수록 회귀모형의 유용성이 높다고 할 수 있다.

9. SSE SSR SST
회귀 모형과 기존의 데이터를 설명할 떄 사용되는 개념

SST :  총 변동(Total SS: total sum of squares), 개별 y의 편차의 제곱이다. 
이는 y의 평균에서 개별 관측값들을 뺀 값들에 대한 제곱의 합이다.
SSE : 설명된 변동( Model SS : model sum of squares), 회귀식 추정 y의 편차제곱의 합이다.
이는 회귀 추정선에서 y의 평균을 뺀 값들에 대한 제곱의 합이다.  
SSR : 잔차의 제곱(Residual sum of squares), 설명 안된 변동에 대한 값이다.
개별 실제 데이터에서 예측 값을 뺀 제곱의 합이다.

관계식으로는, SST = SSR + SSE로 설명할 수 있다.

10. 탐욕적 알고리즘 / 게으른 알고리즘 
탐욕적 기법
욕심쟁이 알고리즘이라고도 불림, 
미리 정한 기준에 따라서 매번 가장 좋아 보이는 답을 선택하는 알고리즘 동적 계획법과 마찬가지로 최적화 문제를 푸는데 사용

게으른 알고리즘 (간단함) 
함수가 오직 지역적으로 근사하고 모든 계산이 분류될 때까지 연기되는 학습 
또는 게으른 학습의 일종이다. k-NN 알고리즘은 가장 간단한 기계 학습 알고리즘에 속한다.
학습이라는 것에 속할 수도 없이 단순 비교로 분류하기 때문임

